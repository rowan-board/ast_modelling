{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1841417/1397664197.py:11: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  base_data = pickle.load(f)\n",
      "/tmp/ipykernel_1841417/1397664197.py:15: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  dreadd_data = pickle.load(f)\n",
      "/tmp/ipykernel_1841417/1397664197.py:19: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  new_data = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "# packages and scripts\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import scripts.functions as sf\n",
    "\n",
    "# load data\n",
    "with open('data/raw/baseline_data.pkl', 'rb') as f:\n",
    "    base_data = pickle.load(f)\n",
    "\n",
    "# load data\n",
    "with open('data/raw/dreadd_data.pkl', 'rb') as f:\n",
    "    dreadd_data = pickle.load(f)\n",
    "\n",
    "# load data\n",
    "with open('data/raw/new_data.pkl', 'rb') as f:\n",
    "    new_data = pickle.load(f)\n",
    "\n",
    "# add condition column\n",
    "dreadd_data['condition'] = dreadd_data['Animal'].apply(lambda x: 'GH DREADDs' if x in ['AMA369', 'AMA422', 'AMA423', 'AMA424'] \n",
    "                                         else 'GH' if x in ['AMA370', 'AMA371']\n",
    "                                         else 'ISO DREADDs')\n",
    "\n",
    "base_data['condition'] = base_data['Animal'].apply(lambda x: 'GH' if x in ['AMA448', 'AMA449'] else ('ISO' if x in ['AMA450', 'AMA451', 'AMA391'] else None))\n",
    "\n",
    "new_data['condition'] = new_data['Animal'].apply(lambda x: 'GH' if x in ['AMA461', 'AMA459'] else ('ISO'))\n",
    "\n",
    "# base data has duplicate rows for AMA391 so drop them\n",
    "base_data = base_data.drop_duplicates()\n",
    "\n",
    "# put datasets in list so we can loop through them later\n",
    "datasets = [base_data, dreadd_data, new_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run consistent function on list of datasets and bind\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i] = sf.consistent(datasets[i])\n",
    "\n",
    "binded_data = pd.concat(datasets)\n",
    "\n",
    "## save clean datasets as pickle files\n",
    "with open('data/raw/binded_data.pkl', 'wb') as file:\n",
    "    pickle.dump(binded_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove alfredos mice\n",
    "data = binded_data[~binded_data['animal'].str.contains('GG')].copy()\n",
    "\n",
    "# get a list of mice and blocks, we'll loop over these later\n",
    "mice = data.animal.unique()\n",
    "blocks = data.block.unique()\n",
    "blocks = [b for b in blocks if b != 'NoRule']\n",
    "\n",
    "# remove na from correct? col\n",
    "data.dropna(subset = ['correct?'], inplace = True)\n",
    "\n",
    "# correct some stimuli names so they are consistent for the model\n",
    "# please don't change order as regex searches and replaces substrings\n",
    "# e.g car is substring of cardboard, so creates carddboard, which we the change to just card\n",
    "data = data.replace('Metal Strip', 'MetalStrip', regex = True)\n",
    "data = data.replace('Metal strip', 'MetalStrip', regex = True)\n",
    "data = data.replace('Poppuri', 'Popurri', regex = True)\n",
    "data = data.replace('Car', 'Card', regex = True)\n",
    "data = data.replace('Carddboard', 'Card', regex = True)\n",
    "data = data.replace('WhBedd', 'WhBed', regex = True)\n",
    "data = data.replace('Whbed', 'WhBed', regex = True)\n",
    "data = data.replace('Turmeric', 'Tumeric', regex = True)\n",
    "data = data.replace('Tumeric ', 'Tumeric', regex = True)\n",
    "data = data.replace('Cardd', 'Card', regex = True)\n",
    "\n",
    "# now run the function to clean the data\n",
    "clean_data = sf.clean_ied(data)\n",
    "\n",
    "# now lets standardize the features\n",
    "clean_data = sf.standardize_features(clean_data)\n",
    "\n",
    "# if you want to remove the 2nd day from each block\n",
    "#clean_data = clean_data[clean_data['day'] == '1']\n",
    "\n",
    "# now lets save the clean data tabular data in csv format \n",
    "clean_data.to_csv('data/clean/clean_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now run through function to build the datalist\n",
    "\n",
    "## split by group\n",
    "mice_ghd = clean_data.loc[clean_data['condition'] == 'GH DREADDs']['mouse'].unique()\n",
    "ghc = ['GH', 'GH mCherry']\n",
    "mice_ghc = clean_data.loc[clean_data['condition'].isin(ghc)]['mouse'].unique()\n",
    "mice_isod = clean_data.loc[clean_data['condition'] == 'ISO DREADDs']['mouse'].unique()\n",
    "isoc = ['ISO', 'ISO mCherry']\n",
    "mice_isoc = clean_data.loc[clean_data['condition'].isin(isoc)]['mouse'].unique()\n",
    "\n",
    "# all mice\n",
    "mice = clean_data['mouse'].unique()\n",
    "\n",
    "## run function to wrangle the data into a format the RL models will use \n",
    "clean_model_data_ghd = [sf.make_ast_model_data(clean_data, mouse) for mouse in mice_ghd]\n",
    "clean_model_data_isod = [sf.make_ast_model_data(clean_data, mouse) for mouse in mice_isod]\n",
    "clean_model_data_ghc = [sf.make_ast_model_data(clean_data, mouse) for mouse in mice_ghc]\n",
    "clean_model_data_isoc = [sf.make_ast_model_data(clean_data, mouse) for mouse in mice_isoc]\n",
    "\n",
    "# all mice\n",
    "clean_model_data = [sf.make_ast_model_data(clean_data, mouse) for mouse in mice]\n",
    "\n",
    "## save clean datasets as pickle files\n",
    "with open('data/modelling/datalists/ghd_r0.pkl', 'wb') as file:\n",
    "    pickle.dump(clean_model_data_ghd, file)\n",
    "with open('data/modelling/datalists/isod_r0.pkl', 'wb') as file:\n",
    "    pickle.dump(clean_model_data_isod, file)\n",
    "with open('data/modelling/datalists/ghc_r0.pkl', 'wb') as file:\n",
    "    pickle.dump(clean_model_data_ghc, file)\n",
    "with open('data/modelling/datalists/isoc_r0.pkl', 'wb') as file:\n",
    "    pickle.dump(clean_model_data_isoc, file)\n",
    "\n",
    "# save all\n",
    "with open('data/modelling/datalists/all_r0.pkl', 'wb') as file:\n",
    "    pickle.dump(clean_model_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: Last 8 choices are all rewarded!\n",
      "Passed: Inital dimension coded correctly!\n"
     ]
    }
   ],
   "source": [
    "# let's check the data \n",
    "sf.sanity_check(clean_model_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
